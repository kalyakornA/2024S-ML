# **机器学习概论 实验三：集成学习 实验报告**
> 李文赢 计05 2020080108

## **实验简介**
本实验目的是实现两个集成学习算法：Bagging 和 AdaBoost，以及两种基分类器：SVM 和 DecisionTree，共4种组合，资源数据为某产品评论评分

## **代码设计思路**
### **数据预处理**
在 `get_dataset` 函数中实现数据预处理
- 接受两个参数 `train_ratio` 和 `random_seed` 用于划分数据集
- 使用 `pandas` 读取 csv 文件，并取出 `reviewText` 和 `overall` 两个数据字段
- 通过 `train_test_split` 函数将数据集划分为训练集和测试集
- 使用 `TfidfVectorizer` 对文本数据进行特征提取，将评论文本转换为 TF-IDF 特征表示
- 最后，函数返回了训练集的 TF-IDF 特征表示 `x_train_tfidf`、测试集的 TF-IDF 特征表示 `x_test_tfidf`，以及对应的训练集标签 `y_train` 和测试集标签 `y_test`

### **Bagging 集成学习算法**
`BaggingClassifier` 类实现了 Bagging 集成学习算法
- `fit` 训练方法，通过循环 `n_estimators` 次，对训练数据进行有放回抽样，然后用基分器进行训练，最后把将训练好的基分器存储在 `estimators` 中为进行预测
- `predict` 预测方法，用于对数据进行分类预测，对每个存储的基分类器进行预测，然后取预测结果的平均值作为最终的预测结果

### **AdaBoost 集成学习算法**
`AdaBoostClassifier` 类实现了 AdaBoost 集成学习算法
- `fit` 训练方法，在每次迭代中通过对训练数据进行有放回抽样，使用子集拟合一个基分类器，预测这个子集上的评价值，并计算预测错误率。权重 `alpha` 表示这个弱分类器在最终分类器中的重要性，然后每次更新样本权重，以便下一个弱分类器能够关注之前被错误分类的样本
- `predict` 预测方法
  - 对于每个训练好的弱分类器使用当前弱分类器对待预测的数据 X 进行预测，将每个弱分类器的预测结果保存在一个列表中
  - 将各个弱分类器的预测结果按照样本顺序转置为一个二维数组，其中每行表示一个样本，在该行中，每个元素是对应弱分类器的预测结果，对于每个样本，根据每个弱分类器的预测结果，计算加权中位数作为最终的预测值
  - 对于每个样本的预测结果，按照预测值的大小进行排序，得到排序后的索引列表，根据弱分类器的权重 `alphas` 的对数值，计算加权中位数。加权中位数是指找到使得累计权重超过总权重一半的位置，该位置即为中位数的位置
  - 将每个样本的加权中位数作为最终的预测结果


## **模型评估结果**

| 模型 | MAE | MSE | RMSE |
| --- | --- | --- | --- |
| Baseline SVM | 0.5273181818181818 | 1.0116818181818181 | 1.005823949894721 |
| Baseline DTree | 0.7532223900192812 | 1.2563000948275521 | 1.1208479356395995 |
| Bagging SVM | 0.5323409090909091 | 0.9239659090909091 | 0.9612314544847714 |
| Bagging DTree | 0.7567727272727273 | 1.3449090909090908 | 1.1597021561198766 |
| AdaBoost SVM | 0.5565 | 1.0697727272727273 | 1.0342981810255336 |
| AdaBoost DTree | 0.8201363636363637 | 1.779409090909091 | 1.3339449354861284 |

## **实验分析**
模型结果表格显示了不同模型在回归任务中的表现，使用了三个评估指标：平均绝对误差（MAE）、均方误差（MSE）和均方根误差（RMSE）

从上面的结果表中可以观察到，集成学习算法能够提高基分类器的性能
- **Bagging SVM**：比 Baseline SVM 改进了一点
- **Bagging DTree**： 在 MAE 和 MSE 上相较于 Baseline DTree 有轻微的改进，但 RMSE 稍微有所增加。这可能是因为 Bagging 对于过拟合较为有效，但对于简单模型如决策树，提升有限
- **AdaBoost SVM**： 在所有指标中相较于 Baseline SVM 有轻微的恶化，但相较于 Bagging SVM 有轻微的改进。AdaBoost 试图通过调整样本权重提高错误样本的权重以改进模型性能，但在这个数据集上可能没有产生显著效果
- **AdaBoost DTree**： 在所有指标中相较于 Baseline DTree 有明显的恶化，而且相较于 Bagging DTree 也没有改进。AdaBoost 对于容易受到噪声干扰的数据和弱分类器表现一般的情况下可能不够有效

因此，针对这个数据集，在这些模型中，SVM 表现最佳，而决策树表现较差，Bagging 和 AdaBoost 在这个数据集上并没有带来明显的性能提升，可能需要更多的调优或者尝试其他模型来改进预测性能